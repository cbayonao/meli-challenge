# üöÄ Quick Start Guide

Get your Meli Challenge project up and running in minutes! This guide covers the fastest path to a working web scraping system.

## ‚ö° 5-Minute Setup

### **Prerequisites Check**
```bash
# Check if you have the required software
python --version          # Should be 3.11+
docker --version          # Docker should be installed
docker-compose --version  # Docker Compose should be installed
aws --version            # AWS CLI should be installed
```

### **1. Clone and Setup**
```bash
# Clone the repository
git clone git@github.com:cbayonao/meli-challenge.git
cd meli-challenge

# Install Python dependencies
uv pip install -e .

# Copy environment template
cp env.example .env
```

### **2. Configure Environment**
Edit `.env` file with your credentials:
```bash
# AWS Configuration
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_key_here
AWS_DEFAULT_REGION=us-east-1

# Service Configuration
DYNAMODB_TABLE_NAME=meli-products
SQS_QUEUE_URL=https://sqs.us-east-1.amazonaws.com/your-queue-url
ZYTE_API_KEY=your_zyte_api_key_here

# Scrapy Configuration
SCRAPY_LOG_LEVEL=INFO
MAX_PAGES=5
MAX_ITEMS=100
```

### **3. Run with Docker (Recommended)**
```bash
# Setup Docker environment
./setup-docker.sh

# Start the application
make up

# Check status
make status
```

### **4. Verify Installation**
```bash
# Check if spiders are available
docker-compose exec meli-crawler scrapy list

# Should show:
# meli-uy-identify
# meli-uy-collect
```

## üê≥ Docker Quick Commands

### **Essential Commands**
```bash
make up              # Start all services
make down            # Stop all services
make logs            # View logs
make status          # Check service status
make restart         # Restart services
```

### **Development Commands**
```bash
make dev-up          # Start development environment
make dev-shell       # Access development container
make dev-logs        # View development logs
make dev-down        # Stop development environment
```

### **Production Commands**
```bash
make prod-up         # Start production environment
make prod-logs       # View production logs
make prod-down       # Stop production environment
```

## üï∑Ô∏è Running Spiders

### **Quick Spider Test**
```bash
# Run identification spider (discovers products)
docker-compose exec meli-crawler scrapy crawl meli-uy-identify

# Run collection spider (extracts details)
docker-compose exec meli-crawler scrapy crawl meli-uy-collect
```

### **Spider with Limits**
```bash
# Run with custom limits (for testing)
docker-compose exec meli-crawler scrapy crawl meli-uy-identify \
  -a max_pages=2 \
  -a max_items=10

# Run collection spider with retry settings
docker-compose exec meli-crawler scrapy crawl meli-uy-collect \
  -a max_batches=5 \
  -a max_messages_per_batch=3
```

## üß™ Quick Testing

### **Run All Tests**
```bash
# Run complete test suite
make test

# Run specific test categories
make test-unit          # Unit tests only
make test-spiders       # Spider tests only
make test-pipelines     # Pipeline tests only
```

### **Test with Coverage**
```bash
# Run tests with coverage report
make test-coverage

# Generate detailed test report
make test-report
```

## üìä Monitor Your System

### **Check Logs**
```bash
# View real-time logs
make logs

# View specific service logs
docker-compose logs -f meli-crawler

# View development logs
make dev-logs
```

### **Check Status**
```bash
# Check service status
make status

# Check Docker containers
docker-compose ps

# Check resource usage
docker stats
```

## üîß Quick Configuration

### **Spider Settings**
```bash
# Environment variables for spider limits
export MAX_PAGES=10
export MAX_ITEMS=500
export MAX_BATCHES=50

# Run spider with new settings
docker-compose exec meli-crawler scrapy crawl meli-uy-identify
```

### **Pipeline Configuration**
```python
# In your spider file
custom_settings = {
    'ITEM_PIPELINES': {
        'meli_crawler.pipelines.ValidationPipeline': 100,
        'meli_crawler.pipelines.PriceNormalizationPipeline': 200,
        'meli_crawler.pipelines.SellerNormalizationPipeline': 500,
        'meli_crawler.pipelines.CreateSellerIdUrlIdPipeline': 600,
        'meli_crawler.pipelines.DynamoDBPipeline': 700,
        'meli_crawler.pipelines.SQSPipeline': 800,
    }
}
```

## üö® Common Issues & Quick Fixes

### **Issue: Import Errors**
```bash
# Solution: Install in development mode
uv pip install -e .

# Or activate virtual environment
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```

### **Issue: AWS Credentials**
```bash
# Solution: Configure AWS CLI
aws configure

# Or set environment variables
export AWS_ACCESS_KEY_ID=your_key
export AWS_SECRET_ACCESS_KEY=your_secret
```

### **Issue: Docker Permission Errors**
```bash
# Solution: Add user to docker group
sudo usermod -aG docker $USER
# Then logout and login again
```

### **Issue: Port Already in Use**
```bash
# Solution: Check what's using the port
lsof -i :8000

# Kill the process or change port in docker-compose.yml
```

## üìà Performance Tuning

### **Quick Performance Settings**
```bash
# Increase concurrency
export SCRAPY_CONCURRENT_REQUESTS=32

# Reduce delays
export SCRAPY_DOWNLOAD_DELAY=0.5

# Enable caching
export SCRAPY_HTTPCACHE_ENABLED=True
```

### **Resource Limits**
```bash
# In docker-compose.yml
services:
  meli-crawler:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
```

## üîç Debug Mode

### **Enable Debug Logging**
```bash
# Set debug level
export SCRAPY_LOG_LEVEL=DEBUG

# Run spider with debug output
docker-compose exec meli-crawler scrapy crawl meli-uy-identify -L DEBUG
```

### **Debug Specific Component**
```bash
# Test specific pipeline
docker-compose exec meli-crawler python -c "
from meli_crawler.pipelines import ValidationPipeline
pipeline = ValidationPipeline()
result = pipeline.process_item({'title': 'test', 'pub_url': 'https://example.com'}, None)
print(result)
"
```

## üöÄ Next Steps

### **After Quick Start**
1. **Review Architecture**: Read `ARCHITECTURE.md` for system design
2. **Explore Tests**: Run `make test` to understand functionality
3. **Customize Settings**: Modify `.env` for your environment
4. **Scale Up**: Increase limits and concurrency settings
5. **Deploy to AWS**: Use `deploy.sh` for cloud deployment

### **Advanced Features**
- **Custom Pipelines**: Add your own data processing logic
- **New Spiders**: Create spiders for other websites
- **Monitoring**: Set up CloudWatch dashboards
- **CI/CD**: Configure GitHub Actions for automation

## üìö Quick Reference

### **Essential Files**
```
.env                    # Environment configuration
docker-compose.yml      # Docker services
Makefile               # Build commands
tests/                 # Test suite
meli_crawler/          # Main application
```

### **Key Commands**
```bash
make up                # Start services
make test              # Run tests
make logs              # View logs
make status            # Check status
make down              # Stop services
```

### **Environment Variables**
```bash
AWS_ACCESS_KEY_ID      # AWS access key
AWS_SECRET_ACCESS_KEY  # AWS secret key
DYNAMODB_TABLE_NAME   # DynamoDB table
SQS_QUEUE_URL         # SQS queue URL
ZYTE_API_KEY          # Zyte API key
```

---

**üéâ You're all set!** Your Meli Challenge web scraping system is now running and ready to extract data from MercadoLibre Uruguay.

For detailed information, check out:
- **`README.md`** - Complete project documentation
- **`ARCHITECTURE.md`** - System architecture details
- **`tests/README.md`** - Testing guide

Happy Scraping! üï∑Ô∏è‚ú®
